{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3838ee",
   "metadata": {},
   "source": [
    "Program 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bcfa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['tokenization', 'process', 'breaking', 'text', 'words', 'phrases', 'stemming', 'lemmatization', 'techniques', 'used', 'reduce', 'words', 'base', 'form']\n",
      "\n",
      "Lemmatization:\n",
      "['tokenization', 'process', 'breaking', 'text', 'word', 'phrase', 'stemming', 'lemmatization', 'technique', 'used', 'reduce', 'word', 'base', 'form']\n",
      "\n",
      "Stemming:\n",
      "['token', 'process', 'break', 'text', 'word', 'phrase', 'stem', 'lemmat', 'techniqu', 'use', 'reduc', 'word', 'base', 'form']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "def preprocess_text(text):\n",
    " # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    " # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    " # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmas\n",
    "def stem(tokens):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "def main():\n",
    " # Sample text\n",
    "    text = \"Tokenization is the process of breaking down text into words and phrases. Stemming and Lemmatization are techniques used to reduce words to their base form.\"\n",
    "\n",
    "     # Preprocess text\n",
    "    tokens = preprocess_text(text)\n",
    "    print(\"tokens:\",tokens)\n",
    "     # Lemmatization\n",
    "    lemmas = lemmatize(tokens)\n",
    "    print(\"Lemmatization:\")\n",
    "    print(lemmas)\n",
    "     # Stemming\n",
    "    stems = stem(tokens)\n",
    "    print(\"\\nStemming:\")\n",
    "    print(stems)\n",
    "if __name__ == \"__main__\": \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d561589",
   "metadata": {},
   "source": [
    "Program 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae4ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the financial dataset\n",
    "data = pd.read_csv(\"financial_dataset.csv\")\n",
    "# Assuming the dataset has two columns: \"text\" containing the text data and \"sentiment\" containing \n",
    "#sentiment labels\n",
    "X = data['text']\n",
    "y = data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Vectorize the text data using N-gram model\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2)) # You can adjust the n-gram range\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "# Train the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "# Predict sentiment on the test set\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69192c3",
   "metadata": {},
   "source": [
    "Program 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52bd93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srika\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ed6fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "text_data = [\n",
    " \"This is the first document.\",\n",
    " \"This document is the second document.\",\n",
    " \"And this is the third one.\",\n",
    " \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f5b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One Hot Encoding:\n",
      "[[0 0 0 0 0 1 1 1 1 0 0 1 1]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 1 1]\n",
      " [0 1 0 1 0 0 1 0 0 1 1 0 1]\n",
      " [1 0 1 1 0 0 1 0 1 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# i) One Hot Encoding\n",
    "def one_hot_encoding(text_data):\n",
    "    unique_words = set(\" \".join(text_data).split())\n",
    "    encoded_data = []\n",
    "    for text in text_data:\n",
    "        encoded_text = [1 if word in text else 0 for word in unique_words]\n",
    "        encoded_data.append(encoded_text)\n",
    "    return np.array(encoded_data)\n",
    "one_hot_encoded = one_hot_encoding(text_data)\n",
    "print(\"One Hot Encoding:\")\n",
    "print(one_hot_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa84d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words (BOW):\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# ii) Bag of Words (BOW)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_features = vectorizer.fit_transform(text_data)\n",
    "print(\"\\nBag of Words (BOW):\")\n",
    "print(bow_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2083d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n-grams:\n",
      "[[0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0]\n",
      " [0 0 2 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0]\n",
      " [1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0]\n",
      " [0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# iii) n-grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "ngram_features = ngram_vectorizer.fit_transform(text_data)\n",
    "print(\"\\nn-grams:\")\n",
    "print(ngram_features.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41e3bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tf-Idf:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# iv) Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(text_data)\n",
    "print(\"\\nTf-Idf:\")\n",
    "print(tfidf_features.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dff91ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Features:\n",
      "[[27]\n",
      " [37]\n",
      " [26]\n",
      " [27]]\n"
     ]
    }
   ],
   "source": [
    "# v) Custom features (e.g., length of documents)\n",
    "custom_features = np.array([[len(doc)] for doc in text_data])\n",
    "print(\"\\nCustom Features:\")\n",
    "print(custom_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d3f27ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec (Word Embedding) Features:\n",
      "[[-6.65289070e-03  3.31369741e-03  4.08868713e-04  2.44881405e-04\n",
      "  -5.88463910e-04 -2.59218062e-03  3.18747014e-03  5.55393519e-03\n",
      "  -2.48023192e-03 -3.41991754e-03  1.00939209e-03 -5.74333710e-04\n",
      "  -4.32855962e-03 -2.97620900e-05 -1.54151581e-04 -1.86934939e-03\n",
      "   1.87192135e-03  6.28794252e-04 -3.34958662e-03 -1.97452726e-03\n",
      "   6.29672548e-04 -2.17557978e-03  8.12448654e-03  3.46868369e-03\n",
      "  -1.32092997e-03 -1.70423693e-04  8.27728480e-04  1.99035089e-03\n",
      "  -2.87406426e-03  2.06327345e-03  3.14927706e-03 -3.12824314e-03\n",
      "  -3.18707607e-04 -6.51314203e-03  2.79311393e-03  2.53376341e-03\n",
      "   5.44173876e-03  5.51073055e-04  8.93780962e-04  2.49114865e-03\n",
      "   2.61284830e-03 -2.20044097e-03 -5.72596490e-03 -1.55718997e-04\n",
      "   1.92174525e-03  6.04374327e-05  7.91336061e-04  2.67658220e-03\n",
      "   1.48482504e-03 -1.77320562e-05  2.14161794e-03 -1.79007486e-03\n",
      "   7.77226523e-04 -2.61274923e-04 -1.91269221e-03 -2.16178130e-04\n",
      "   3.14616645e-03 -1.51300768e-03 -1.33311469e-03 -2.83975154e-04\n",
      "   6.82370155e-04  3.30017123e-04  2.11696210e-03 -4.26681805e-03\n",
      "   1.35116815e-03  7.36578251e-04  3.35936132e-03  3.86734377e-03\n",
      "  -1.91979972e-03 -2.11127481e-04  8.47946270e-04  1.55021541e-03\n",
      "   1.52969093e-03  3.28939478e-03  7.13645772e-04  8.61627806e-04\n",
      "  -3.33618978e-03  3.17021599e-03  9.31327697e-04 -3.19909537e-03\n",
      "  -3.22103081e-03 -3.17895785e-04  4.62956214e-03 -1.81684620e-03\n",
      "  -2.74393009e-03 -3.95658985e-03  3.66323930e-03  3.62912251e-04\n",
      "  -2.75213225e-03 -1.86016853e-03  1.49975729e-03  3.55360069e-04\n",
      "   3.89443245e-04 -3.49399890e-03  5.82279125e-03  8.64184112e-04\n",
      "   1.38464908e-03 -4.15094476e-03 -1.47293159e-03  3.37768521e-04]\n",
      " [-3.05007561e-03  3.03780031e-03  3.32541578e-03 -1.21483659e-04\n",
      "  -1.24764556e-04 -3.30740842e-03  1.98407006e-03  6.85360702e-03\n",
      "  -5.04148600e-04 -5.63225290e-03  1.70957448e-03 -1.92572642e-03\n",
      "  -3.86051484e-03 -1.35194778e-03  7.70922459e-04 -3.19330231e-03\n",
      "   1.66899839e-03 -1.00072904e-03 -3.39492480e-03 -2.68489029e-03\n",
      "  -1.09133718e-03 -1.60682108e-03  5.76833310e-03  1.26356038e-03\n",
      "  -2.26620235e-03 -4.36741597e-04 -1.94102712e-03  3.26459366e-03\n",
      "  -6.44951302e-04  3.13167856e-03 -2.83049187e-04 -3.91772809e-03\n",
      "  -1.87940270e-04 -3.78301484e-03  9.59999161e-05  1.75637461e-03\n",
      "   3.95375816e-03 -1.45929772e-03 -1.41937591e-04  9.42060433e-04\n",
      "   1.81194438e-04  2.72466103e-04 -3.26020736e-03 -6.24690729e-04\n",
      "  -1.24203973e-03 -8.39584682e-04  3.16493423e-03  1.25053327e-03\n",
      "  -1.01766421e-03  3.47247650e-03  2.67279078e-03 -4.24565608e-03\n",
      "   2.46527459e-04  2.77698186e-04 -2.26352573e-03  2.71156896e-04\n",
      "   3.80364689e-03  1.57504401e-03  2.17107357e-04  1.50101213e-03\n",
      "  -1.56108534e-03  5.06309851e-04  2.85643525e-03 -3.78622231e-03\n",
      "  -1.57596078e-05  2.09751818e-03  3.98740591e-03  2.89800880e-03\n",
      "  -3.86031833e-03 -9.82257538e-04  2.09207856e-03  2.95363739e-03\n",
      "   2.95731775e-03  2.00126227e-03  4.27163526e-04  6.60107005e-04\n",
      "  -2.57985271e-03  1.15432229e-03 -2.22720276e-03 -1.79128011e-03\n",
      "  -3.12814885e-03  3.60024441e-03  2.15843716e-03  2.48296157e-04\n",
      "  -8.55257094e-04 -2.03268253e-03  1.91160885e-03 -1.77285599e-03\n",
      "  -8.44350652e-05  5.85242466e-04  1.96441263e-03  1.10311412e-04\n",
      "  -1.02576474e-03 -2.85158027e-03  6.17120415e-03  1.20427506e-03\n",
      "   2.63467286e-04 -3.76052293e-03 -1.04455370e-03  6.96758972e-04]\n",
      " [-2.78480933e-03  1.28072803e-03  1.30271725e-03  3.57292965e-03\n",
      "   4.10743378e-04 -2.75190757e-03  9.22966574e-04  3.89556959e-03\n",
      "  -4.94354125e-03 -1.92943355e-03  2.68267561e-03 -4.20946302e-03\n",
      "   2.65057199e-03  1.19234540e-03  2.74546887e-03  2.26927106e-03\n",
      "   2.46763602e-03  6.14898338e-04 -3.13333259e-03 -5.52342087e-03\n",
      "   2.73606071e-04  1.16434600e-03  3.81175056e-03 -4.96752933e-03\n",
      "   4.34077159e-03 -4.93702828e-04 -1.34325866e-03  1.97735452e-03\n",
      "  -1.67649565e-03 -3.79864039e-04  3.15302215e-03 -2.24373862e-03\n",
      "   3.42300371e-03  1.89737603e-03 -1.56044308e-03  2.28676014e-03\n",
      "   2.86331121e-03  2.19650101e-03  1.71548745e-03 -2.01256873e-04\n",
      "  -1.12879416e-03  1.48486171e-03 -4.72988980e-03  6.16949983e-04\n",
      "  -1.20565889e-03  9.58712597e-04 -9.50876158e-04  2.34619365e-03\n",
      "  -9.34309035e-04  2.43711029e-03 -2.85372022e-04 -9.16567806e-05\n",
      "  -3.38953454e-03  2.90542841e-04  1.62353273e-03 -2.04305374e-03\n",
      "   7.33558787e-04 -1.35091052e-03  1.20466505e-03  1.56906771e-03\n",
      "  -4.05863393e-03 -4.41300217e-05  3.45243770e-03 -2.05106614e-03\n",
      "  -4.86907410e-03  2.76652467e-03 -5.73569385e-04 -9.55305353e-04\n",
      "  -1.14775053e-03  1.64502149e-03  4.32249944e-04  3.42412014e-03\n",
      "   1.09983806e-03  1.06019084e-03 -6.58494828e-04 -1.15519622e-04\n",
      "   1.88846549e-03 -3.39471851e-03 -1.01297267e-03 -1.22790074e-03\n",
      "  -1.13514165e-04 -5.15974534e-04  2.34389119e-03  2.86770030e-03\n",
      "  -9.73796879e-04 -1.00682897e-03 -1.78134022e-03 -4.10853280e-03\n",
      "   3.04183667e-03  2.73985253e-03 -1.39047171e-03  2.58963555e-03\n",
      "   3.41931987e-03 -1.00512016e-05  4.81797522e-03  2.36608088e-03\n",
      "   8.56563041e-04 -2.41162415e-04  3.11721186e-03 -1.05100253e-03]\n",
      " [ 1.74367055e-03 -8.68972682e-04 -1.93493514e-04  2.93102232e-03\n",
      "  -5.07914112e-04  1.50294113e-03  2.87550734e-03  1.95410056e-03\n",
      "  -5.25067840e-03  3.28857102e-04  3.33140692e-04  2.33105238e-04\n",
      "   5.94583689e-05  3.26480577e-03 -1.48771901e-03 -2.10257969e-03\n",
      "   3.79444892e-03  4.65765921e-03 -3.15659563e-03 -7.54166115e-03\n",
      "   4.39284282e-04 -7.82012939e-04  4.32578940e-03 -1.25521212e-03\n",
      "   1.02349662e-03  3.02228320e-04  5.19805471e-04  3.55234742e-03\n",
      "  -5.82543341e-03  1.41370948e-03  9.83433565e-04  1.22087472e-03\n",
      "   3.05563823e-04 -4.58441954e-03 -2.37513334e-03 -1.84859079e-03\n",
      "  -1.54373702e-03 -1.80838595e-03 -2.54996913e-03 -2.10772525e-03\n",
      "   5.86982642e-04 -1.64801290e-03 -2.41609453e-03  1.54198823e-03\n",
      "   4.44495445e-03  2.14960979e-04 -3.49989301e-03 -1.15155277e-03\n",
      "   2.03727977e-03 -1.47351675e-04 -2.32885825e-03 -8.16538930e-04\n",
      "  -7.85643206e-05 -3.24097835e-03 -3.11552128e-03 -3.49416840e-03\n",
      "  -3.66872409e-03 -4.99979733e-03 -3.48435668e-03 -5.13345760e-04\n",
      "   3.11409193e-03 -3.39984009e-03  1.87960605e-03  1.00331160e-03\n",
      "  -2.67989351e-03  4.65676980e-03  2.51674384e-04  5.02156839e-03\n",
      "  -4.52307053e-03  1.35516166e-03  1.00060017e-03  5.09968447e-03\n",
      "   3.28540045e-04 -2.39632835e-04  1.39656418e-03  2.12706951e-03\n",
      "   5.51989069e-03  2.57661846e-03  3.64999374e-04 -3.93182877e-03\n",
      "  -1.65854045e-03 -4.03716322e-03 -3.08640557e-03  1.87159982e-03\n",
      "  -2.28370377e-03 -4.07998450e-03  4.83568246e-03 -1.10757968e-03\n",
      "  -4.41467564e-04  2.56528256e-05 -7.76166562e-05 -4.75944020e-04\n",
      "   1.23110134e-03 -4.51194588e-04  3.51448101e-03  5.09851146e-04\n",
      "   1.91939960e-03 -2.39616237e-03  2.46154470e-03  7.37562601e-04]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'text_data' is a list of strings\n",
    "word2vec_model = Word2Vec([doc.split() for doc in text_data], min_count=1)\n",
    "word2vec_features = np.array([np.mean([word2vec_model.wv[word] for word in doc.split()], axis=0) for doc in text_data]) \n",
    "print(\"\\nWord2Vec (Word Embedding) Features:\")\n",
    "print(word2vec_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57a72b",
   "metadata": {},
   "source": [
    "Program 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "050decd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e16c1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for each topic:\n",
      "Topic 1: soccer basketball tennis cricket baseball\n",
      "Topic 2: cricket tennis soccer baseball basketball\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"baseball soccer basketball\",\n",
    "    \"soccer basketball tennis\",\n",
    "    \"tennis cricket\",\n",
    "    \"cricket soccer\"\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer to convert text data into a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Apply Latent Semantic Analysis (LSA)\n",
    "lsa = TruncatedSVD(n_components=2)\n",
    "X_lsa = lsa.fit(X)\n",
    "\n",
    "# Extract the components/topics\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "topic_matrix = np.array([lsa.components_[i] / np.linalg.norm(lsa.components_[i]) for i in range(lsa.components_.shape[0])])\n",
    "\n",
    "# Print the topics\n",
    "print(\"Top terms for each topic:\")\n",
    "for i, topic in enumerate(topic_matrix):\n",
    "    top_indices = topic.argsort()[-5:][::-1]  # Get the top 5 terms for each topic\n",
    "    top_terms = [terms[index] for index in top_indices]\n",
    "    print(f\"Topic {i + 1}: {' '.join(top_terms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb162b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
